{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÄ\n"
     ]
    }
   ],
   "source": [
    "# python„ÅßÁµµÊñáÂ≠ó„ÇíÂá∫ÂäõüòØ\n",
    "print('\\U0001F600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: 2.1.2+cu121\n",
      "cuda: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "print(f'pytorch: {torch.__version__}')\n",
    "print(f'cuda: {torch.cuda.is_available()}')\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "from src import lightning_model\n",
    "from src.data import get_dataloaders\n",
    "\n",
    "# Fix seed\n",
    "pl.seed_everything(0, workers=True)\n",
    "\n",
    "\n",
    "DATA_PATH = 'Flickr8k'\n",
    "IMAGE_ENCODER_NAME = 'resnet18'\n",
    "TEXT_ENCODER_NAME = 'albert-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    print('Downloading the dataset from Kaggle...')\n",
    "    #os.system('wget\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(data_dir=DATA_PATH, tokenizer_name=TEXT_ENCODER_NAME, batch_size=16, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | image_encoder    | ImageEncoder   | 11.2 M\n",
      "1 | text_encoder     | TextEncoder    | 11.7 M\n",
      "2 | image_projection | ProjectionHead | 657 K \n",
      "3 | text_projection  | ProjectionHead | 920 K \n",
      "----------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.752    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/clip/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/root/anaconda3/envs/clip/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: You called `self.log('step', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'step': ...})` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|‚ñé         | 60/2023 [01:39<54:23,  0.60it/s, v_num=3, train/loss_step=2.770]  "
     ]
    }
   ],
   "source": [
    "model = lightning_model.CLIPModel(image_encoder_alias=IMAGE_ENCODER_NAME, text_encoder_alias=TEXT_ENCODER_NAME)\n",
    "trainer = pl.Trainer(max_epochs=10, enable_checkpointing=True, callbacks=[lightning_model.OverrideEpochStepCallback(), EarlyStopping(monitor='val/loss', patience=30, verbose=False)])\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torchinfo/torchinfo.py:216: UserWarning: Half precision is not supported with input_size parameter, and may output incorrect results. Try passing input_data directly.\n",
      "  validate_user_params(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "BaseModel                                          [256, 7]                  --\n",
       "‚îú‚îÄViT: 1-1                                         [256, 7]                  --\n",
       "‚îÇ    ‚îî‚îÄPatchEmbedding: 2-1                         [256, 64, 128]            8,192\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                            [256, 128, 8, 8]          4,736\n",
       "‚îÇ    ‚îî‚îÄCustomTransformerEncoder: 2-2               [256, 64, 128]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        1,851,392\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-3                             [256, 7]                  --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-3                         [256, 128]                256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-4                            [256, 128]                16,512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGEGLU: 3-5                             [256, 64]                 --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-6                            [256, 7]                  455\n",
       "====================================================================================================\n",
       "Total params: 1,881,543\n",
       "Trainable params: 1,881,543\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 555.96\n",
       "====================================================================================================\n",
       "Input size (MB): 1.18\n",
       "Forward/backward pass size (MB): 887.36\n",
       "Params size (MB): 3.75\n",
       "Estimated Total Size (MB): 892.29\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, dtypes=[torch.bfloat16], input_size=(next(iter(train_loader))[0].shape[0], next(iter(train_loader))[0].shape[1], next(iter(train_loader))[0].shape[2], next(iter(train_loader))[0].shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7e69cd6707b12ef7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7e69cd6707b12ef7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard  --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
